{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e43ec680-ca4f-4b00-b5f5-cbc529093946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top N: 5, Top K: 5, Average AUC: 0.46863237139272274, Average MRR: 0.6089469281736335, Average nDCG@5: 0.4232946577290306\n",
      "Top N: 10, Top K: 5, Average AUC: 0.5026655592347661, Average MRR: 0.6352693213586035, Average nDCG@5: 0.4631348729810971\n",
      "Top N: 15, Top K: 5, Average AUC: 0.5054794579046709, Average MRR: 0.6437630221078019, Average nDCG@5: 0.48291304034035015\n",
      "Top N: 20, Top K: 5, Average AUC: 0.5072723110573589, Average MRR: 0.649763134056631, Average nDCG@5: 0.4953617591480573\n",
      "Top N: 25, Top K: 5, Average AUC: 0.5104260530546013, Average MRR: 0.6528766014347919, Average nDCG@5: 0.5031011159658589\n",
      "Top N: 30, Top K: 5, Average AUC: 0.5135811246946943, Average MRR: 0.6543131653173135, Average nDCG@5: 0.5081315050883414\n",
      "Top N: 35, Top K: 5, Average AUC: 0.5162782660793884, Average MRR: 0.6540869228374682, Average nDCG@5: 0.5108023929886552\n",
      "Top N: 40, Top K: 5, Average AUC: 0.518431074324796, Average MRR: 0.6558471034594212, Average nDCG@5: 0.5139606058604936\n",
      "Top N: 45, Top K: 5, Average AUC: 0.5197236182187479, Average MRR: 0.6567348274594245, Average nDCG@5: 0.5165635285570637\n",
      "Top N: 50, Top K: 5, Average AUC: 0.5224325656939609, Average MRR: 0.6568513010771716, Average nDCG@5: 0.5186347801917233\n",
      "Best Parameters: {'top_n': 50, 'top_k': 5}\n",
      "Best AUC: 0.5224325656939609, Best MRR: 0.6568513010771716, Best nDCG@5: 0.5186347801917233\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# Load the DataFrame\n",
    "file_name = 'articles_small.parquet'\n",
    "df = pd.read_parquet(file_name)\n",
    "history_df = pd.read_parquet('history_small_train.parquet')\n",
    "behaviors_df = pd.read_parquet('behaviors_small_train.parquet')\n",
    "history_validation_df = pd.read_parquet('history_small_validation.parquet')\n",
    "\n",
    "# Ensure required columns\n",
    "required_columns = ['article_id', 'published_time', 'total_inviews', 'total_pageviews', 'total_read_time', 'premium', 'sentiment_label']\n",
    "if not all(col in df.columns for col in required_columns):\n",
    "    raise KeyError(f\"Required columns {required_columns} not found in the dataset.\")\n",
    "\n",
    "# Convert and filter published time\n",
    "df['published_time'] = pd.to_datetime(df['published_time'])\n",
    "df = df[df['published_time'] > '2023-02-23']\n",
    "\n",
    "# Drop rows with any null values\n",
    "df = df.dropna(subset=['category_str', 'total_inviews', 'total_read_time', 'sentiment_label', 'topics'])\n",
    "\n",
    "# Encode category_str using OneHotEncoder\n",
    "category_encoder = OneHotEncoder()\n",
    "category_encoded = category_encoder.fit_transform(df[['category_str']]).toarray()\n",
    "\n",
    "# Normalize total_inviews and total_read_time\n",
    "scaler = MinMaxScaler()\n",
    "numerical_data = scaler.fit_transform(df[['total_inviews', 'total_read_time']])\n",
    "\n",
    "# Encode sentiment_label\n",
    "sentiment_mapping = {'Negative': -1, 'Neutral': 0, 'Positive': 1}\n",
    "df['sentiment_label'] = df['sentiment_label'].map(sentiment_mapping)\n",
    "\n",
    "# Vectorize topics using TfidfVectorizer\n",
    "topics = df['topics'].apply(lambda x: ' '.join(x))\n",
    "vectorizer = TfidfVectorizer()\n",
    "topics_encoded = vectorizer.fit_transform(topics).toarray()\n",
    "\n",
    "# Combine all features into a single feature set\n",
    "combined_features = np.hstack((category_encoded, numerical_data, df[['sentiment_label']].values.reshape(-1, 1), topics_encoded))\n",
    "\n",
    "# Compute cosine similarity for articles\n",
    "article_cosine_sim_matrix = cosine_similarity(combined_features)\n",
    "\n",
    "# Filter articles for recommendations\n",
    "filtered_df = df[['article_id', 'total_pageviews', 'sentiment_label']].copy()\n",
    "filtered_df.loc[:, 'normalized_read_time'] = filtered_df['total_pageviews'] / filtered_df['total_pageviews'].max()\n",
    "filtered_df.loc[:, 'sentiment_label_value'] = filtered_df['sentiment_label'].map({1: 1, 0: 0.5, -1: 0})\n",
    "filtered_df.loc[:, 'combined_score'] = 0.6 * filtered_df['normalized_read_time'] + 0.4 * filtered_df['sentiment_label_value']\n",
    "filtered_df = filtered_df.sort_values('combined_score', ascending=False)\n",
    "filtered_df.loc[:, 'normalized_rank'] = filtered_df['combined_score']\n",
    "\n",
    "# Filter behaviors_basic dataframe based on distinct user_id and non-subscribers\n",
    "filtered_behaviors_df = behaviors_df.drop_duplicates(subset=['user_id'])\n",
    "filtered_behaviors_df = filtered_behaviors_df[['user_id', 'is_subscriber', 'device_type']]\n",
    "\n",
    "# Perform inner join with history_df on user_id\n",
    "merged_df = pd.merge(history_df, filtered_behaviors_df, on='user_id', how='inner')\n",
    "merged_df = merged_df.drop(columns=['scroll_percentage_fixed', 'impression_time_fixed', 'read_time_fixed'])\n",
    "merged_df['is_subscriber'] = merged_df['is_subscriber'].astype(int)\n",
    "\n",
    "# Create a dictionary with user_id as keys and clicked articles as values\n",
    "user_articles = merged_df.set_index('user_id')['article_id_fixed'].to_dict()\n",
    "\n",
    "\n",
    "# Create a combined feature string for each user\n",
    "user_features = {\n",
    "    user_id: ' '.join(map(str, articles)) + f\" subscriber_{row['is_subscriber']} device_{row['device_type']}\"\n",
    "    for user_id, articles, row in zip(user_articles.keys(), user_articles.values(), merged_df.to_dict('records'))\n",
    "}\n",
    "\n",
    "# Create a matrix of users and their clicked articles including additional features\n",
    "vectorizer = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "user_feature_matrix = vectorizer.fit_transform(user_features.values())\n",
    "\n",
    "# Calculate cosine similarity between users\n",
    "user_cosine_sim = cosine_similarity(user_feature_matrix)\n",
    "\n",
    "# Convert cosine similarity matrix to DataFrame\n",
    "user_ids = list(user_features.keys())\n",
    "user_cosine_sim_df = pd.DataFrame(user_cosine_sim, index=user_ids, columns=user_ids)\n",
    "\n",
    "# Function to get top N neighbors for a user\n",
    "def get_top_neighbors(user_id, top_n=5):\n",
    "    if user_id not in user_cosine_sim_df.index:\n",
    "        return []\n",
    "    neighbors = user_cosine_sim_df[user_id].nlargest(top_n + 1).iloc[1:].index\n",
    "    return neighbors\n",
    "\n",
    "# Function to get article recommendations for a user, incorporating article ranking\n",
    "def recommend_articles(user_id, top_n=10):\n",
    "    neighbors = get_top_neighbors(user_id, top_n)\n",
    "    neighbor_articles = [article for neighbor in neighbors for article in user_articles[neighbor]]\n",
    "    neighbor_articles_counts = pd.Series(neighbor_articles).value_counts()\n",
    "    \n",
    "    # Create a DataFrame for articles and their counts\n",
    "    article_scores_df = neighbor_articles_counts.reset_index()\n",
    "    article_scores_df.columns = ['article_id', 'interaction_count']\n",
    "    \n",
    "    # Merge with the filtered_df to get the rank of the articles\n",
    "    article_scores_df = article_scores_df.merge(filtered_df[['article_id', 'normalized_rank']], on='article_id', how='left')\n",
    "    \n",
    "    # Calculate a combined score\n",
    "    article_scores_df['combined_score'] = article_scores_df['interaction_count'] * (1 - article_scores_df['normalized_rank'])\n",
    "    \n",
    "    # Sort articles based on the combined score\n",
    "    article_scores_df = article_scores_df.sort_values('combined_score', ascending=False)\n",
    "    \n",
    "    # Get the top N recommended articles\n",
    "    recommended_articles = article_scores_df['article_id'].head(top_n)\n",
    "    return recommended_articles.tolist()\n",
    "\n",
    "# Get recommendations for each user in the merged_df\n",
    "merged_df['recommended_articles'] = merged_df['user_id'].apply(lambda user_id: recommend_articles(user_id, top_n=15))\n",
    "\n",
    "# Ensure the history_validation_df only contains user IDs that exist in the merged_df\n",
    "history_validation_df = history_validation_df[history_validation_df['user_id'].isin(merged_df['user_id'])]\n",
    "\n",
    "# Function to calculate AUC for a user\n",
    "def calculate_auc(user_id):\n",
    "    actual_articles = history_validation_df[history_validation_df['user_id'] == user_id]['article_id_fixed'].values\n",
    "    if actual_articles.size == 0:\n",
    "        return 0\n",
    "    actual_articles = set(actual_articles[0])\n",
    "    predicted_articles = merged_df[merged_df['user_id'] == user_id]['recommended_articles'].values[0]\n",
    "    y_true = [1 if article in actual_articles else 0 for article in predicted_articles]\n",
    "    y_scores = list(range(len(predicted_articles), 0, -1))\n",
    "    if len(set(y_true)) == 1:  # Avoid cases where all true labels are the same\n",
    "        return 0\n",
    "    return roc_auc_score(y_true, y_scores)\n",
    "\n",
    "# Function to calculate MRR for a user\n",
    "def calculate_mrr(user_id):\n",
    "    actual_articles = history_validation_df[history_validation_df['user_id'] == user_id]['article_id_fixed'].values\n",
    "    if actual_articles.size == 0:\n",
    "        return 0\n",
    "    actual_articles = set(actual_articles[0])\n",
    "    predicted_articles = merged_df[merged_df['user_id'] == user_id]['recommended_articles'].values[0]\n",
    "    for rank, article in enumerate(predicted_articles, start=1):\n",
    "        if article in actual_articles:\n",
    "            return 1 / rank\n",
    "    return 0\n",
    "\n",
    "# Function to calculate nDCG@K for a user\n",
    "def calculate_ndcg(user_id, k=15):\n",
    "    actual_articles = history_validation_df[history_validation_df['user_id'] == user_id]['article_id_fixed'].values\n",
    "    if actual_articles.size == 0:\n",
    "        return 0\n",
    "    actual_articles = set(actual_articles[0])\n",
    "    predicted_articles = merged_df[merged_df['user_id'] == user_id]['recommended_articles'].values[0][:k]\n",
    "    dcg = sum((1 / (i + 1) if article in actual_articles else 0) for i, article in enumerate(predicted_articles))\n",
    "    idcg = sum(1 / (i + 1) for i in range(min(len(actual_articles), k)))\n",
    "    return dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "# Function to tune parameters and calculate metrics\n",
    "def tune_parameters(merged_df, history_validation_df, user_articles):\n",
    "    top_neighbors_values = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "    top_recommendations_values = [5]\n",
    "    \n",
    "    best_auc = 0\n",
    "    best_mrr = 0\n",
    "    best_ndcg5 = 0\n",
    "    best_params = {}\n",
    "\n",
    "    for top_n in top_neighbors_values:\n",
    "        for top_k in top_recommendations_values:\n",
    "            merged_df['recommended_articles'] = merged_df['user_id'].apply(lambda user_id: recommend_articles(user_id, top_n=top_n))\n",
    "            merged_df['auc'] = merged_df['user_id'].apply(calculate_auc)\n",
    "            merged_df['mrr'] = merged_df['user_id'].apply(calculate_mrr)\n",
    "            merged_df['ndcg@5'] = merged_df['user_id'].apply(lambda user_id: calculate_ndcg(user_id, k=top_k))\n",
    "            \n",
    "            average_auc = merged_df['auc'].mean()\n",
    "            average_mrr = merged_df['mrr'].mean()\n",
    "            average_ndcg5 = merged_df['ndcg@5'].mean()\n",
    "\n",
    "            if average_auc > best_auc and average_mrr > best_mrr and average_ndcg5 > best_ndcg5:\n",
    "                best_auc = average_auc\n",
    "                best_mrr = average_mrr\n",
    "                best_ndcg5 = average_ndcg5\n",
    "                best_params = {'top_n': top_n, 'top_k': top_k}\n",
    "            \n",
    "            print(f\"Top N: {top_n}, Top K: {top_k}, Average AUC: {average_auc}, Average MRR: {average_mrr}, Average nDCG@5: {average_ndcg5}\")\n",
    "\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Best AUC: {best_auc}, Best MRR: {best_mrr}, Best nDCG@5: {best_ndcg5}\")\n",
    "\n",
    "# Run the tuning function\n",
    "tune_parameters(merged_df, history_validation_df, user_articles)\n",
    "\n",
    "# Save the filtered articles dataframe to parquet\n",
    "filtered_df.to_parquet('filtered_articles_by_total_read_time.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe77a98e-eb35-4c78-8aee-119d0aeccc7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
