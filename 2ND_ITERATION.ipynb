{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cd5c71c-d661-4068-a7e3-690412a7fb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       article_id                                  title  \\\n",
      "11059     9647575      Kvinderne der kan sparke din røv!   \n",
      "11508     9667501         Advarsel! Bliv inden døre i år   \n",
      "11943     9685759  Realitystjerner: - Sådan scorer du os   \n",
      "11948     9685856      Realitystjerner: Det tænder vi på   \n",
      "12063     9690920                  Dansk og dødbringende   \n",
      "\n",
      "                                                subtitle  last_modified_time  \\\n",
      "11059  Blot fordi der står supermodel, sanger eller s... 2023-06-29 06:47:06   \n",
      "11508                                FESTIVALGUIDE 2023: 2023-06-29 06:47:22   \n",
      "11943  Vi har spurgte en række realitystjerner, hvord... 2023-06-29 06:47:38   \n",
      "11948  Find ud af, om du har det, der skal til, hvis ... 2023-06-29 06:47:38   \n",
      "12063  Hos Dansk Industri har man store forhåbninger ... 2023-06-29 06:47:43   \n",
      "\n",
      "       premium                                               body  \\\n",
      "11059     True  Det er sjældent noget, de skilter med, men der...   \n",
      "11508    False  Det er muligt, det bliver en god sommer. Men d...   \n",
      "11943     True  Foråret er kommet, og med det kommer også amor...   \n",
      "11948     True  Nogle tænder på fødder. Andre på smæk i numsen...   \n",
      "12063     True  Supersælger med blåt blod. Danske firmaer er d...   \n",
      "\n",
      "           published_time                                          image_ids  \\\n",
      "11059 2023-04-28 04:32:21  [6024353, 6024353, 8169886, 8368543, 8469593, ...   \n",
      "11508 2023-04-30 06:00:47  [9726684, 9726667, 7258875, 7258874, 9668618, ...   \n",
      "11943 2023-05-05 06:00:43                                          [9740965]   \n",
      "11948 2023-05-11 13:45:25                                          [9740968]   \n",
      "12063 2023-05-04 09:01:20  [9690954, 9690947, 9691036, 9690944, 9690966, ...   \n",
      "\n",
      "          article_type                                                url  \\\n",
      "11059  article_default  https://ekstrabladet.dk/underholdning/udlandke...   \n",
      "11508  article_default  https://ekstrabladet.dk/musik/dkmusiknyt/advar...   \n",
      "11943  article_default  https://ekstrabladet.dk/underholdning/reality/...   \n",
      "11948  article_default  https://ekstrabladet.dk/underholdning/reality/...   \n",
      "12063  article_default  https://ekstrabladet.dk/underholdning/kongelig...   \n",
      "\n",
      "       ...                                      entity_groups  \\\n",
      "11059  ...  [MISC, ORG, PROD, PER, PER, PER, ORG, PROD, PE...   \n",
      "11508  ...  [PER, PER, LOC, PROD, ORG, ORG, PROD, PER, PER...   \n",
      "11943  ...  [PER, PROD, PER, PER, PER, LOC, PER, ORG, PER,...   \n",
      "11948  ...  [LOC, ORG, PER, EVENT, PER, PROD, MISC, PER, P...   \n",
      "12063  ...  [ORG, LOC, LOC, LOC, ORG, ORG, ORG, ORG, PER, ...   \n",
      "\n",
      "                                                  topics category  \\\n",
      "11059                           [Kendt, Livsstil, Sport]      414   \n",
      "11508  [Underholdning, Begivenhed, Musik og lyd, Unde...      498   \n",
      "11943          [Kendt, Livsstil, Underholdning, Reality]      414   \n",
      "11948  [Kendt, Livsstil, Underholdning, Reality, Erotik]      414   \n",
      "12063  [Erhverv, Kendt, Konflikt og krig, Teknologi, ...      414   \n",
      "\n",
      "       subcategory   category_str total_inviews  total_pageviews  \\\n",
      "11059        [432]  underholdning      913674.0          45301.0   \n",
      "11508        [500]          musik     1450787.0         317824.0   \n",
      "11943       [2728]  underholdning      777418.0          30988.0   \n",
      "11948       [2728]  underholdning     1206431.0          93936.0   \n",
      "12063   [429, 430]  underholdning      415532.0          52768.0   \n",
      "\n",
      "       total_read_time  sentiment_score  sentiment_label  \n",
      "11059        2246020.0           0.9136          Neutral  \n",
      "11508       29429770.0           0.7311          Neutral  \n",
      "11943        1591284.0           0.7867          Neutral  \n",
      "11948        4119931.0           0.8745          Neutral  \n",
      "12063        2616030.0           0.9232          Neutral  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "total_pageviews    0\n",
      "total_inviews      0\n",
      "sentiment_label    0\n",
      "total_read_time    0\n",
      "dtype: int64\n",
      "   user_id                                            premium  \\\n",
      "0    10068                                            [False]   \n",
      "1    10200  [False, True, True, False, False, True, False,...   \n",
      "2    10201                                     [False, False]   \n",
      "3    10623  [False, True, False, False, False, False, Fals...   \n",
      "4    10701  [True, False, False, False, False, True, False...   \n",
      "\n",
      "                                        category_str  \\\n",
      "0                                    [underholdning]   \n",
      "1  [bibliotek, nyheder, krimi, nyheder, nyheder, ...   \n",
      "2                                   [nyheder, penge]   \n",
      "3  [nyheder, underholdning, krimi, nationen, nyhe...   \n",
      "4  [nyheder, nyheder, krimi, krimi, nyheder, krim...   \n",
      "\n",
      "                                           read_time  \\\n",
      "0                                            [185.0]   \n",
      "1  [9.0, 42.0, 16.0, 30.0, 30.0, 50.0, 39.0, 27.0...   \n",
      "2                                       [53.0, 87.0]   \n",
      "3  [17.0, 8.0, 46.0, 16.0, 7.0, 7.0, 14.0, 4.0, 2...   \n",
      "4  [27.0, 6.0, 15.0, 20.0, 9.0, 18.0, 55.0, 5.0, ...   \n",
      "\n",
      "                                   scroll_percentage  device_type  \\\n",
      "0                                            [100.0]            2   \n",
      "1  [100.0, 100.0, nan, nan, 100.0, nan, nan, nan,...            2   \n",
      "2                                       [nan, 100.0]            1   \n",
      "3  [100.0, nan, nan, 100.0, nan, nan, nan, nan, 1...            2   \n",
      "4  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...            1   \n",
      "\n",
      "                                 article_ids_clicked  is_subscriber  \\\n",
      "0                                          [9779748]          False   \n",
      "1  [9735909, 9755800, 9766721, 9769581, 9771042, ...          False   \n",
      "2                                 [9771568, 9772088]          False   \n",
      "3  [9754160, 9759955, 9769457, 9770288, 9770886, ...          False   \n",
      "4  [9738729, 9754160, 9756785, 9769424, 9769504, ...          False   \n",
      "\n",
      "                                      next_read_time  \n",
      "0                                             [91.0]  \n",
      "1  [7.0, 10.0, 22.0, 3.0, 2.0, 7.0, 38.0, 3.0, 8....  \n",
      "2                                       [80.0, 31.0]  \n",
      "3  [6.0, 4.0, 2.0, 12.0, 3.0, 23.0, nan, 14.0, 2....  \n",
      "4  [8.0, 7.0, 25.0, 64.0, 7.0, 19.0, 8.0, 9.0, 10...  \n",
      "Top N: 5, Top K: 5, Average AUC: 0.3685066023191903, Average MRR: 0.7652143798735394, Average nDCG@5: 0.6102777480144606\n",
      "Top N: 10, Top K: 5, Average AUC: 0.5364473687010385, Average MRR: 0.76794632816149, Average nDCG@5: 0.6562574251322102\n",
      "Top N: 15, Top K: 5, Average AUC: 0.5980480808518044, Average MRR: 0.7682511727906861, Average nDCG@5: 0.6741066087774225\n",
      "Top N: 20, Top K: 5, Average AUC: 0.6275285620802623, Average MRR: 0.7682057113941922, Average nDCG@5: 0.6817953298175108\n",
      "Top N: 25, Top K: 5, Average AUC: 0.6427287168938973, Average MRR: 0.7683698877539517, Average nDCG@5: 0.6860005195941294\n",
      "Top N: 30, Top K: 5, Average AUC: 0.6495223303746146, Average MRR: 0.7686046832508967, Average nDCG@5: 0.6885710916809653\n",
      "Top N: 35, Top K: 5, Average AUC: 0.6527846547990738, Average MRR: 0.7689664856868191, Average nDCG@5: 0.690827558842078\n",
      "Top N: 40, Top K: 5, Average AUC: 0.6522198254076964, Average MRR: 0.7688373558546129, Average nDCG@5: 0.691832988160397\n",
      "Top N: 45, Top K: 5, Average AUC: 0.6527262721449318, Average MRR: 0.7690591532471281, Average nDCG@5: 0.6927082743163446\n",
      "Top N: 50, Top K: 5, Average AUC: 0.6521030403034356, Average MRR: 0.7690693268184174, Average nDCG@5: 0.693357522348174\n",
      "Top N: 55, Top K: 5, Average AUC: 0.6505225668605005, Average MRR: 0.7689078535603235, Average nDCG@5: 0.6936628205937365\n",
      "Top N: 60, Top K: 5, Average AUC: 0.6494463309711472, Average MRR: 0.7687340167244855, Average nDCG@5: 0.6941745865630609\n",
      "Best Parameters: {'top_n': 35, 'top_k': 5}\n",
      "Best AUC: 0.6527846547990738, Best MRR: 0.7689664856868191, Best nDCG@5: 0.690827558842078\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "# Load the DataFrame\n",
    "file_name = 'articles_small.parquet'\n",
    "articles_df = pd.read_parquet(file_name)\n",
    "history_df = pd.read_parquet('history_small_train.parquet')\n",
    "behaviors_df = pd.read_parquet('behaviors_small_train.parquet')\n",
    "history_validation_df = pd.read_parquet('history_small_validation.parquet')\n",
    "\n",
    "\n",
    "# Filter articles by the date range from April 27 to June 8, 2023\n",
    "filtered_articles_df = articles_df[\n",
    "    (articles_df['published_time'] >= '2023-04-27') & (articles_df['published_time'] <= '2023-06-08')\n",
    "]\n",
    "\n",
    "print(filtered_articles_df.head())\n",
    "\n",
    "filtered_articles_df = filtered_articles_df.dropna()\n",
    "\n",
    "# Check for null values in specified columns\n",
    "null_values = filtered_articles_df[['total_pageviews', 'total_inviews', 'sentiment_label', 'total_read_time']].isnull().sum()\n",
    "print(null_values)\n",
    "\n",
    "# Explode article_ids_clicked to prepare for the join\n",
    "behaviors_df_exploded = behaviors_df.explode('article_ids_clicked')\n",
    "\n",
    "# Drop rows with NaN values after exploding\n",
    "behaviors_df_exploded = behaviors_df_exploded.dropna(subset=['article_ids_clicked'])\n",
    "\n",
    "# Perform the inner join\n",
    "merged_df = pd.merge(\n",
    "    filtered_articles_df,\n",
    "    behaviors_df_exploded,\n",
    "    left_on='article_id',\n",
    "    right_on='article_ids_clicked',\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# List of columns to keep, with user_id first\n",
    "columns_to_keep = [\n",
    "    'user_id', 'premium', 'category_str', 'read_time', 'scroll_percentage',\n",
    "    'device_type', 'article_ids_clicked', 'is_subscriber', 'next_read_time'\n",
    "]\n",
    "\n",
    "# Keep only the desired columns and rearrange them\n",
    "filtered_merged_df = merged_df[columns_to_keep]\n",
    "\n",
    "# Group by user_id and aggregate other columns appropriately\n",
    "grouped_df = filtered_merged_df.groupby('user_id').agg({\n",
    "    'premium': list,\n",
    "    'category_str': list,\n",
    "    'read_time': list,\n",
    "    'scroll_percentage': list,\n",
    "    'device_type': 'first',\n",
    "    'article_ids_clicked': list,\n",
    "    'is_subscriber': 'first',\n",
    "    'next_read_time': list\n",
    "}).reset_index()\n",
    "\n",
    "# Drop rows with any null values in grouped_df\n",
    "grouped_df = grouped_df.dropna()\n",
    "\n",
    "print(grouped_df.head())\n",
    "\n",
    "# Create a dictionary with user_id as keys and clicked articles as values\n",
    "user_articles = merged_df.groupby('user_id')['article_ids_clicked'].apply(list).to_dict()\n",
    "\n",
    "# Create a combined feature string for each user\n",
    "user_features = {\n",
    "    user_id: ' '.join(map(str, articles)) + f\" subscriber_{row['is_subscriber']} device_{row['device_type']}\"\n",
    "    for user_id, articles, row in zip(user_articles.keys(), user_articles.values(), grouped_df.to_dict('records'))\n",
    "}\n",
    "\n",
    "# Create a matrix of users and their clicked articles including additional features\n",
    "vectorizer = CountVectorizer(token_pattern=r\"(?u)\\b\\w+\\b\")\n",
    "user_feature_matrix = vectorizer.fit_transform(user_features.values())\n",
    "\n",
    "# Calculate cosine similarity between users\n",
    "user_cosine_sim = cosine_similarity(user_feature_matrix)\n",
    "\n",
    "# Convert cosine similarity matrix to DataFrame\n",
    "user_ids = list(user_features.keys())\n",
    "user_cosine_sim_df = pd.DataFrame(user_cosine_sim, index=user_ids, columns=user_ids)\n",
    "\n",
    "# Function to get top N neighbors for a user\n",
    "def get_top_neighbors(user_id, top_n=5):\n",
    "    if user_id not in user_cosine_sim_df.index:\n",
    "        return []\n",
    "    neighbors = user_cosine_sim_df[user_id].nlargest(top_n + 1).iloc[1:].index\n",
    "    return neighbors\n",
    "\n",
    "# Function to get article recommendations for a user, incorporating article ranking\n",
    "def recommend_articles(user_id, top_n=10):\n",
    "    neighbors = get_top_neighbors(user_id, top_n)\n",
    "    neighbor_articles = [article for neighbor in neighbors for article in user_articles[neighbor]]\n",
    "    neighbor_articles_counts = pd.Series(neighbor_articles).value_counts()\n",
    "    \n",
    "    # Create a DataFrame for articles and their counts\n",
    "    article_scores_df = neighbor_articles_counts.reset_index()\n",
    "    article_scores_df.columns = ['article_id', 'interaction_count']\n",
    "    \n",
    "    # Merge with the filtered_df to get the rank of the articles\n",
    "    filtered_df = articles_df[['article_id', 'total_pageviews', 'sentiment_label', 'sentiment_score']].copy()\n",
    "\n",
    "    # Normalize the total_pageviews column\n",
    "    filtered_df['normalized_total_pageviews'] = filtered_df['total_pageviews'] / filtered_df['total_pageviews'].max()\n",
    "\n",
    "    # Create a mapping for sentiment_label\n",
    "    sentiment_mapping = {1: 1, 0: 0.5, -1: -1}\n",
    "\n",
    "    # Map the sentiment_label to sentiment_label_value and multiply by sentiment_score\n",
    "    filtered_df['sentiment_label_value'] = filtered_df['sentiment_label'].map(sentiment_mapping)\n",
    "    filtered_df['adjusted_sentiment_score'] = filtered_df['sentiment_score'] * filtered_df['sentiment_label_value']\n",
    "\n",
    "    # Normalize the adjusted_sentiment_score column\n",
    "    filtered_df['normalized_adjusted_sentiment_score'] = (filtered_df['adjusted_sentiment_score'] - filtered_df['adjusted_sentiment_score'].min()) / (filtered_df['adjusted_sentiment_score'].max() - filtered_df['adjusted_sentiment_score'].min())\n",
    "\n",
    "    # Combine the normalized_total_pageviews and normalized_adjusted_sentiment_score into a combined_score\n",
    "    filtered_df['combined_score'] = 0.8 * filtered_df['normalized_total_pageviews'] + 0.2 * filtered_df['normalized_adjusted_sentiment_score']\n",
    "\n",
    "    # Sort the DataFrame based on combined_score in descending order\n",
    "    filtered_df = filtered_df.sort_values('combined_score', ascending=False)\n",
    "\n",
    "    # Normalize the combined_score to get the normalized_rank\n",
    "    filtered_df['normalized_rank'] = filtered_df['combined_score'] / filtered_df['combined_score'].max()\n",
    "\n",
    "    # Merge the normalized_rank back into article_scores_df\n",
    "    article_scores_df = article_scores_df.merge(filtered_df[['article_id', 'normalized_rank']], on='article_id', how='left')\n",
    "\n",
    "    # Define the weights for interaction count and normalized rank\n",
    "    interaction_count_weight = 0.9\n",
    "    normalized_rank_weight = 0.1\n",
    "\n",
    "# Calculate the combined score with the weights\n",
    "    article_scores_df['combined_score'] = (interaction_count_weight * article_scores_df['interaction_count']) + (normalized_rank_weight * (1 - article_scores_df['normalized_rank']))\n",
    "\n",
    "# Normalize the combined score if needed\n",
    "    article_scores_df['combined_score'] = article_scores_df['combined_score'] / article_scores_df['combined_score'].max()\n",
    "\n",
    "# Sort articles based on the combined score\n",
    "    article_scores_df = article_scores_df.sort_values('combined_score', ascending=False)\n",
    "\n",
    "    # Get the top N recommended articles\n",
    "    recommended_articles = article_scores_df['article_id'].head(top_n)\n",
    "\n",
    "    return recommended_articles.tolist()\n",
    "\n",
    "# Get recommendations for each user in the grouped_df\n",
    "grouped_df['recommended_articles'] = grouped_df['user_id'].apply(lambda user_id: recommend_articles(user_id, top_n=15))\n",
    "\n",
    "# Ensure the history_validation_df only contains user IDs that exist in the grouped_df\n",
    "history_validation_df = history_validation_df[history_validation_df['user_id'].isin(grouped_df['user_id'])]\n",
    "\n",
    "# Function to calculate AUC for a user\n",
    "def calculate_auc(user_id):\n",
    "    actual_articles = history_validation_df[history_validation_df['user_id'] == user_id]['article_id_fixed'].values\n",
    "    if actual_articles.size == 0:\n",
    "        return 0\n",
    "    actual_articles = set(actual_articles[0])\n",
    "    predicted_articles = grouped_df[grouped_df['user_id'] == user_id]['recommended_articles'].values[0]\n",
    "    y_true = [1 if article in actual_articles else 0 for article in predicted_articles]\n",
    "    y_scores = list(range(len(predicted_articles), 0, -1))\n",
    "    if len(set(y_true)) == 1:  # Avoid cases where all true labels are the same\n",
    "        return 0\n",
    "    return roc_auc_score(y_true, y_scores)\n",
    "\n",
    "# Function to calculate MRR for a user\n",
    "def calculate_mrr(user_id):\n",
    "    actual_articles = history_validation_df[history_validation_df['user_id'] == user_id]['article_id_fixed'].values\n",
    "    if actual_articles.size == 0:\n",
    "        return 0\n",
    "    actual_articles = set(actual_articles[0])\n",
    "    predicted_articles = grouped_df[grouped_df['user_id'] == user_id]['recommended_articles'].values[0]\n",
    "    for rank, article in enumerate(predicted_articles, start=1):\n",
    "        if article in actual_articles:\n",
    "            return 1 / rank\n",
    "    return 0\n",
    "\n",
    "# Function to calculate nDCG@K for a user\n",
    "def calculate_ndcg(user_id, k=15):\n",
    "    actual_articles = history_validation_df[history_validation_df['user_id'] == user_id]['article_id_fixed'].values\n",
    "    if actual_articles.size == 0:\n",
    "        return 0\n",
    "    actual_articles = set(actual_articles[0])\n",
    "    predicted_articles = grouped_df[grouped_df['user_id'] == user_id]['recommended_articles'].values[0][:k]\n",
    "    dcg = sum((1 / (i + 1) if article in actual_articles else 0) for i, article in enumerate(predicted_articles))\n",
    "    idcg = sum(1 / (i + 1) for i in range(min(len(actual_articles), k)))\n",
    "    return dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "# Function to tune parameters and calculate metrics\n",
    "def tune_parameters(grouped_df, history_validation_df):\n",
    "    top_neighbors_values = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60]\n",
    "    top_recommendations_values = [5]\n",
    "    \n",
    "    best_auc = 0\n",
    "    best_mrr = 0\n",
    "    best_ndcg5 = 0\n",
    "    best_params = {}\n",
    "\n",
    "    for top_n in top_neighbors_values:\n",
    "        for top_k in top_recommendations_values:\n",
    "            grouped_df['recommended_articles'] = grouped_df['user_id'].apply(lambda user_id: recommend_articles(user_id, top_n=top_n))\n",
    "            grouped_df['auc'] = grouped_df['user_id'].apply(calculate_auc)\n",
    "            grouped_df['mrr'] = grouped_df['user_id'].apply(calculate_mrr)\n",
    "            grouped_df['ndcg@5'] = grouped_df['user_id'].apply(lambda user_id: calculate_ndcg(user_id, k=top_k))\n",
    "            \n",
    "            average_auc = grouped_df['auc'].mean()\n",
    "            average_mrr = grouped_df['mrr'].mean()\n",
    "            average_ndcg5 = grouped_df['ndcg@5'].mean()\n",
    "\n",
    "            if average_auc > best_auc and average_mrr > best_mrr and average_ndcg5 > best_ndcg5:\n",
    "                best_auc = average_auc\n",
    "                best_mrr = average_mrr\n",
    "                best_ndcg5 = average_ndcg5\n",
    "                best_params = {'top_n': top_n, 'top_k': top_k}\n",
    "            \n",
    "            print(f\"Top N: {top_n}, Top K: {top_k}, Average AUC: {average_auc}, Average MRR: {average_mrr}, Average nDCG@5: {average_ndcg5}\")\n",
    "\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "    print(f\"Best AUC: {best_auc}, Best MRR: {best_mrr}, Best nDCG@5: {best_ndcg5}\")\n",
    "\n",
    "# Run the tuning function\n",
    "tune_parameters(grouped_df, history_validation_df)\n",
    "\n",
    "# Save the grouped DataFrame to parquet\n",
    "grouped_df.to_parquet('grouped_df.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e16e23f-e2d5-471b-935c-46152eea39a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
